import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from . import help_functions as hf
import os

#determine if request is GET or POST
def get_or_post(response):
	# Check the request type
	if response.request.method == "GET":
		return "GET"
	elif response.request.method == "POST":
		return "POST"
	else:
		return response.request.method

#create a file given response text
def createFile(text, filename):	
	if not os.path.exists(filename):
		with open(filename, 'w', encoding='utf-8') as file:
			file.write(text)
			print(f"File '{filename}' created successfully.")
		
		file.close()
	else:
		print(f"File '{filename}' already exists in the directory.")


#crawls every web page given URL recursively
def crawl_website(url):
	target_url = url
	
	# List to store visited URLs
	visited_urls = []
	visited_js = []

    # Function to recursively visit each page
	def visit_page(url):
		print(url)
        # Add the current URL to the visited URLs list
		visited_urls.append(url)

        # Send a GET request to the current URL
		response = requests.get(url)
		
		# Parse URL with '/' character and '?'
		# Get filename
		path = url.split('//')[-1]
		filename = path.split('?')[0].replace('/','@')
		filename = '[HTML]' + filename + '.txt'
		
		# Get parameters
		params = ''
		if '?' in path:
			params = path.split('?')[1]
		
		# Create a file as HTML code with a given response.text and filename
		createFile(response.text, filename)		
		
        # Parse the HTML content of the response
		soup = BeautifulSoup(response.text, 'html.parser')
		
		# Find all the script tags on the page
		for script in soup.find_all('script'):
			# Get the src attribute of the script tag
			src = script.get('src')
			if src is None:
				continue
				
			print("[SRC] "+src)
			
			#Check if src is path from target URL			
			if "http://" in src or "https://" in src:
				if target_url in src:
					visited_js.append(src)
					
					print("[JOINED_SRC1] "+src)
				
					res_js = requests.get(src)
					srcpath = src.split('//')[-1]
					js_filename = srcpath.split('?')[0].replace('/','@')
					js_filename = '[JS]' + js_filename + '.txt'
					createFile(res_js.text, js_filename)
			else:
				absolute_jsurl = urljoin(url, src)
				
				print("[JOINED_SRC2] "+absolute_jsurl)
				
				visited_js.append(absolute_jsurl)

				res_js = requests.get(absolute_jsurl)
				srcpath = absolute_jsurl.split('//')[-1]
				js_filename = srcpath.split('?')[0].replace('/','@')
				js_filename = '[JS]' + js_filename + '.txt'
				createFile(res_js.text, js_filename)
				

        # Find all the anchor tags on the page
		for link in soup.find_all('a'):
            # Get the href attribute of the anchor tag
			href = link.get('href')

            # Skip if href is None or empty
			if not href:
				continue

            # Join the href with the base URL
			absolute_url = urljoin(url, href)

            # Parse the absolute URL
			parsed_url = urlparse(absolute_url)

            # Skip if the URL is not from the same domain or is already visited
			if parsed_url.netloc != urlparse(url).netloc or absolute_url in visited_urls:
				continue

            # Recursively visit the page
			visit_page(absolute_url)

    # Start crawling from the given URL
	visit_page(url)
	
	print("######## DONE ##########")

    # Print the visited URLs
	for visited_url in visited_urls:
		print("############### URL ##################")
		print(visited_url)
		
	for visited_js_page in visited_js:
		print("################# JS ################")
		print(visited_js_page)


#crawl_website('https://webhacking.kr/')






